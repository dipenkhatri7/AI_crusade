{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okQdUOf2ovBS"
      },
      "source": [
        "#Running MMS-TTS inference in Colab\n",
        "In this notebook, we give an example on how to run text-to-speech inference using MMS TTS models.\n",
        "\n",
        "By default, we run inference on a GPU.  If you want to perform CPU inference, go to \"Runtiime\" menu -> \"Change runtime type\" and set \"Hardware accelerator\" to \"None\" before running."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XK2jXLmEpgK5"
      },
      "source": [
        "## 1. Preliminaries\n",
        "This section installs necessary python packages for the other sections. Run it first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vGyb3dGWpmks",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8a0fd92-f234-433d-e0c4-7cd61db780ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'vits'...\n",
            "remote: Enumerating objects: 81, done.\u001b[K\n",
            "remote: Total 81 (delta 0), reused 0 (delta 0), pack-reused 81\u001b[K\n",
            "Receiving objects: 100% (81/81), 3.33 MiB | 6.79 MiB/s, done.\n",
            "Resolving deltas: 100% (22/22), done.\n",
            "Python 3.10.12\n",
            "/content/vits\n",
            "Collecting Cython==0.29.21\n",
            "  Downloading Cython-0.29.21-py2.py3-none-any.whl (974 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m974.2/974.2 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: Cython\n",
            "  Attempting uninstall: Cython\n",
            "    Found existing installation: Cython 0.29.36\n",
            "    Uninstalling Cython-0.29.36:\n",
            "      Successfully uninstalled Cython-0.29.36\n",
            "Successfully installed Cython-0.29.21\n",
            "Collecting librosa==0.8.0\n",
            "  Downloading librosa-0.8.0.tar.gz (183 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from librosa==0.8.0) (3.0.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from librosa==0.8.0) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from librosa==0.8.0) (1.10.1)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from librosa==0.8.0) (1.2.2)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa==0.8.0) (1.3.2)\n",
            "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from librosa==0.8.0) (4.4.2)\n",
            "Collecting resampy>=0.2.2 (from librosa==0.8.0)\n",
            "  Downloading resampy-0.4.2-py3-none-any.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numba>=0.43.0 in /usr/local/lib/python3.10/dist-packages (from librosa==0.8.0) (0.56.4)\n",
            "Requirement already satisfied: soundfile>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from librosa==0.8.0) (0.12.1)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa==0.8.0) (1.7.0)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.43.0->librosa==0.8.0) (0.39.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from numba>=0.43.0->librosa==0.8.0) (67.7.2)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa==0.8.0) (3.10.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa==0.8.0) (23.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa==0.8.0) (2.31.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn!=0.19.0,>=0.14.0->librosa==0.8.0) (3.2.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.9.0->librosa==0.8.0) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.9.0->librosa==0.8.0) (2.21)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa==0.8.0) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa==0.8.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa==0.8.0) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa==0.8.0) (2023.7.22)\n",
            "Building wheels for collected packages: librosa\n",
            "  Building wheel for librosa (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for librosa: filename=librosa-0.8.0-py3-none-any.whl size=201367 sha256=6a53f6637543d0844c09ac961785b7e91f7f5d7a8cb62667ee2bbd8f72707530\n",
            "  Stored in directory: /root/.cache/pip/wheels/bf/b7/85/2f8044306ccec014930aea23ad4852fca9e2584e21c6972bc6\n",
            "Successfully built librosa\n"
          ]
        }
      ],
      "source": [
        "%pwd\n",
        "!git clone https://github.com/jaywalnut310/vits.git\n",
        "!python --version\n",
        "%cd vits/\n",
        "\n",
        "!pip install Cython==0.29.21\n",
        "!pip install librosa==0.8.0\n",
        "!pip install phonemizer==2.2.1\n",
        "!pip install scipy\n",
        "!pip install numpy\n",
        "!pip install torch\n",
        "!pip install torchvision\n",
        "!pip install matplotlib\n",
        "!pip install Unidecode==1.1.1\n",
        "!pip install scipy\n",
        "!pip install fastapi\n",
        "!pip install colabcode\n",
        "!pip install pydub\n",
        "\n",
        "%cd monotonic_align/\n",
        "%mkdir monotonic_align\n",
        "!python3 setup.py build_ext --inplace\n",
        "%cd ../\n",
        "%pwd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KuBzieKbuJKN"
      },
      "source": [
        "## 2. Choose a language and download its checkpoint\n",
        "Find the ISO code for your target language [here](https://dl.fbaipublicfiles.com/mms/tts/all-tts-languages.html). You can find more details about the languages we currently support for TTS in this [table](https://dl.fbaipublicfiles.com/mms/misc/language_coverage_mms.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UtEeQcmwuUaG"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import subprocess\n",
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "\n",
        "def download(lang, tgt_dir=\"./\"):\n",
        "  lang_fn, lang_dir = os.path.join(tgt_dir, lang+'.tar.gz'), os.path.join(tgt_dir, lang)\n",
        "  cmd = \";\".join([\n",
        "        f\"wget https://dl.fbaipublicfiles.com/mms/tts/{lang}.tar.gz -O {lang_fn}\",\n",
        "        f\"tar zxvf {lang_fn}\"\n",
        "  ])\n",
        "  print(f\"Download model for language: {lang}\")\n",
        "  subprocess.check_output(cmd, shell=True)\n",
        "  print(f\"Model checkpoints in {lang_dir}: {os.listdir(lang_dir)}\")\n",
        "  return lang_dir\n",
        "\n",
        "LANG = \"eng\"\n",
        "ckpt_dir = download(LANG)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Load the checkpoint"
      ],
      "metadata": {
        "id": "zexlezYiSWMb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sxi3CXmGqH6r"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Audio\n",
        "import os\n",
        "import re\n",
        "import glob\n",
        "import json\n",
        "import tempfile\n",
        "import math\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import commons\n",
        "import utils\n",
        "import argparse\n",
        "import subprocess\n",
        "from data_utils import TextAudioLoader, TextAudioCollate, TextAudioSpeakerLoader, TextAudioSpeakerCollate\n",
        "from models import SynthesizerTrn\n",
        "from scipy.io.wavfile import write\n",
        "\n",
        "def preprocess_char(text, lang=None):\n",
        "    \"\"\"\n",
        "    Special treatement of characters in certain languages\n",
        "    \"\"\"\n",
        "    print(lang)\n",
        "    if lang == 'ron':\n",
        "        text = text.replace(\"ț\", \"ţ\")\n",
        "    return text\n",
        "\n",
        "class TextMapper(object):\n",
        "    def __init__(self, vocab_file):\n",
        "        self.symbols = [x.replace(\"\\n\", \"\") for x in open(vocab_file, encoding=\"utf-8\").readlines()]\n",
        "        self.SPACE_ID = self.symbols.index(\" \")\n",
        "        self._symbol_to_id = {s: i for i, s in enumerate(self.symbols)}\n",
        "        self._id_to_symbol = {i: s for i, s in enumerate(self.symbols)}\n",
        "\n",
        "    def text_to_sequence(self, text, cleaner_names):\n",
        "        '''Converts a string of text to a sequence of IDs corresponding to the symbols in the text.\n",
        "        Args:\n",
        "        text: string to convert to a sequence\n",
        "        cleaner_names: names of the cleaner functions to run the text through\n",
        "        Returns:\n",
        "        List of integers corresponding to the symbols in the text\n",
        "        '''\n",
        "        sequence = []\n",
        "        clean_text = text.strip()\n",
        "        for symbol in clean_text:\n",
        "            symbol_id = self._symbol_to_id[symbol]\n",
        "            sequence += [symbol_id]\n",
        "        return sequence\n",
        "\n",
        "    def uromanize(self, text, uroman_pl):\n",
        "        iso = \"xxx\"\n",
        "        with tempfile.NamedTemporaryFile() as tf, \\\n",
        "             tempfile.NamedTemporaryFile() as tf2:\n",
        "            with open(tf.name, \"w\") as f:\n",
        "                f.write(\"\\n\".join([text]))\n",
        "            cmd = f\"perl \" + uroman_pl\n",
        "            cmd += f\" -l {iso} \"\n",
        "            cmd +=  f\" < {tf.name} > {tf2.name}\"\n",
        "            os.system(cmd)\n",
        "            outtexts = []\n",
        "            with open(tf2.name) as f:\n",
        "                for line in f:\n",
        "                    line =  re.sub(r\"\\s+\", \" \", line).strip()\n",
        "                    outtexts.append(line)\n",
        "            outtext = outtexts[0]\n",
        "        return outtext\n",
        "\n",
        "    def get_text(self, text, hps):\n",
        "        text_norm = self.text_to_sequence(text, hps.data.text_cleaners)\n",
        "        if hps.data.add_blank:\n",
        "            text_norm = commons.intersperse(text_norm, 0)\n",
        "        text_norm = torch.LongTensor(text_norm)\n",
        "        return text_norm\n",
        "\n",
        "    def filter_oov(self, text):\n",
        "        val_chars = self._symbol_to_id\n",
        "        txt_filt = \"\".join(list(filter(lambda x: x in val_chars, text)))\n",
        "        print(f\"text after filtering OOV: {txt_filt}\")\n",
        "        return txt_filt\n",
        "\n",
        "def preprocess_text(txt, text_mapper, hps, uroman_dir=None, lang=None):\n",
        "    txt = preprocess_char(txt, lang=lang)\n",
        "    is_uroman = hps.data.training_files.split('.')[-1] == 'uroman'\n",
        "    if is_uroman:\n",
        "        with tempfile.TemporaryDirectory() as tmp_dir:\n",
        "            if uroman_dir is None:\n",
        "                cmd = f\"git clone git@github.com:isi-nlp/uroman.git {tmp_dir}\"\n",
        "                print(cmd)\n",
        "                subprocess.check_output(cmd, shell=True)\n",
        "                uroman_dir = tmp_dir\n",
        "            uroman_pl = os.path.join(uroman_dir, \"bin\", \"uroman.pl\")\n",
        "            print(f\"uromanize\")\n",
        "            txt = text_mapper.uromanize(txt, uroman_pl)\n",
        "            print(f\"uroman text: {txt}\")\n",
        "    txt = txt.lower()\n",
        "    txt = text_mapper.filter_oov(txt)\n",
        "    return txt\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "print(f\"Run inference with {device}\")\n",
        "vocab_file = f\"{ckpt_dir}/vocab.txt\"\n",
        "config_file = f\"{ckpt_dir}/config.json\"\n",
        "assert os.path.isfile(config_file), f\"{config_file} doesn't exist\"\n",
        "hps = utils.get_hparams_from_file(config_file)\n",
        "print(hps)\n",
        "text_mapper = TextMapper(vocab_file)\n",
        "net_g = SynthesizerTrn(\n",
        "    len(text_mapper.symbols),\n",
        "    hps.data.filter_length // 2 + 1,\n",
        "    hps.train.segment_size // hps.data.hop_length,\n",
        "    **hps.model)\n",
        "net_g.to(device)\n",
        "_ = net_g.eval()\n",
        "\n",
        "g_pth = f\"{ckpt_dir}/G_100000.pth\"\n",
        "print(f\"load {g_pth}\")\n",
        "\n",
        "_ = utils.load_checkpoint(g_pth, net_g, None)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Generate an audio given text\n",
        "Specify the sentence you want to synthesize and generate the audio"
      ],
      "metadata": {
        "id": "fIiwaWl6SiVy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def getAudio():\n",
        "#   txt = \"\"\"Game_1 was a thrilling match between two evenly matched teams. Both sides started off the game aggressively, with the home team launching a series of attacks in the opening minutes. However, the away team managed to keep the pressure on and eventually took the lead with a well-taken goal. The home team responded well and equalised shortly afterwards, but the away team retook the lead shortly before half-time.\n",
        "\n",
        "# The second half was a tense affair, with both teams having chances to take the lead. The home team had the best chances but were unable to convert them into goals. The away team then managed to break the deadlock with a well-taken goal to make the score 3-1. The home team pushed for an equaliser but were unable to find the back of the net, and the away team held on to secure a hard-fought victory.\"\"\"\n",
        "  txt=\"\"\"Wow, folks, you won't believe the action we're witnessing in this Dota 2 match! The teams are locked in an epic showdown, and the intensity is off the charts. The strategies, the plays, it's all happening right here!\n",
        "\n",
        "We've got some incredible heroes in this game, and they're showcasing their skills like never before. The clashes are explosive, and the team fights are absolutely jaw-dropping!\n",
        "\"\"\"\n",
        "  print(f\"text: {txt}\")\n",
        "  txt = preprocess_text(txt, text_mapper, hps, lang=LANG)\n",
        "  stn_tst = text_mapper.get_text(txt, hps)\n",
        "  with torch.no_grad():\n",
        "      x_tst = stn_tst.unsqueeze(0).to(device)\n",
        "      x_tst_lengths = torch.LongTensor([stn_tst.size(0)]).to(device)\n",
        "      hyp = net_g.infer(\n",
        "          x_tst, x_tst_lengths, noise_scale=.667,\n",
        "          noise_scale_w=0.8, length_scale=1.0\n",
        "      )[0][0,0].cpu().float().numpy()\n",
        "\n",
        "  print(f\"Generated audio\")\n",
        "  return {\"hyp\": hyp,\"hps\":hps}"
      ],
      "metadata": {
        "id": "mpSvjfSCGBDm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fastapi colabcode"
      ],
      "metadata": {
        "id": "UmnuBAqVUik7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Union\n",
        "\n",
        "from fastapi import FastAPI\n",
        "from colabcode import ColabCode\n",
        "from fastapi.staticfiles import StaticFiles\n",
        "from fastapi.responses import FileResponse\n",
        "app = FastAPI()\n",
        "from scipy.io import wavfile\n",
        "from pydub import AudioSegment\n",
        "\n",
        "\n",
        "sample_rate = 44100  # Adjust this to your desired sample rate\n",
        "\n",
        "\n",
        "@app.get(\"/\")\n",
        "async def read_root():\n",
        "\n",
        "    val =getAudio()\n",
        "    hyp = val[\"hyp\"]\n",
        "    hps = val[\"hps\"]\n",
        "    print(type(hyp))\n",
        "    wavfile.write(\"output.wav\", hps.data.sampling_rate, hyp)\n",
        "    # Load the base audio file\n",
        "    audio = AudioSegment.from_file(\"output.wav\")\n",
        "    # Increase pitch and speed for excitement\n",
        "    return FileResponse(\"output.wav\", media_type=\"audio/wav\")\n",
        "\n",
        "\n",
        "\n",
        "@app.get(\"/items/{item_id}\")\n",
        "def read_item(item_id: int, q: Union[str, None] = None):\n",
        "    return {\"item_id\": item_id, \"q\": q}\n",
        "cc = ColabCode(port =5000,code =False)\n",
        "cc.run_app(app=app)"
      ],
      "metadata": {
        "id": "OiQR1UN4yvPs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.getcwd())\n"
      ],
      "metadata": {
        "id": "RS9WvbnEkRSb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scipy\n",
        "!pip install fastapi\n",
        "!pip install colabcode"
      ],
      "metadata": {
        "id": "PmACzK7qhxtM"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}